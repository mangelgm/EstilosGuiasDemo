# Cimaprompter - MÃ³dulo 17: Academic QA Assistant with RAG

**Proyecto Capstone: Building your academic QA assistant**

---

## ğŸ‘¥ Equipo Cimaprompter

**InstituciÃ³n:** Universidad AutÃ³noma de Baja California (UABC)

**Integrantes:**
- Miguel Ãngel GonzÃ¡lez Mandujano
- Monica Valenzuela Delgado
- Karina Caro Corrales
- Juan Francisco Flores Resendiz

**Curso:** FundaciÃ³n TecnolÃ³gica Iberoamericana - AI Course
**MÃ³dulo:** 17 - Complete & Polish

---

## ğŸ“š Resumen del Proyecto

Asistente acadÃ©mico de preguntas y respuestas basado en **RAG (Retrieval Augmented Generation)**. El sistema responde preguntas sobre guÃ­as de estilo de cÃ³digo recuperando informaciÃ³n directamente de documentos PDF, mostrando las fuentes utilizadas y mÃ©tricas de rendimiento en tiempo real.

### EvoluciÃ³n del proyecto

| MÃ³dulo | Fase | Lo que se construyÃ³ |
|--------|------|---------------------|
| 15 | Base | Chat con Gemini + explicabilidad SHAP/LIME |
| 16 | Add Metrics & Testing | RAG con ChromaDB + panel de 4 mÃ©tricas + test dataset |
| 17 | Complete & Polish | CorrecciÃ³n de bugs, mejoras de UX, documentaciÃ³n |

---

## ğŸ”§ Cambios del MÃ³dulo 16 â†’ MÃ³dulo 17

Durante el MÃ³dulo 16 se identificaron 5 problemas mediante pruebas. En el MÃ³dulo 17 se corrigieron todos:

### Fix 1: MÃ©trica de citaciÃ³n corregida
**Problema:** La tasa de citaciÃ³n marcaba 100% siempre porque contaba si el RAG devolviÃ³ chunks, no si el LLM realmente citÃ³ fuentes en el texto.
**SoluciÃ³n:** La mÃ©trica ahora busca patrones de citaciÃ³n explÃ­cita en la respuesta (`[Fuente 1]`, `segÃºn la guÃ­a`, etc.).

### Fix 2: Manejo de preguntas fuera de alcance
**Problema:** Para preguntas off-topic, el sistema intentaba responder usando conocimiento general del LLM en lugar de indicar que no tiene esa informaciÃ³n.
**SoluciÃ³n:** System prompt reforzado con instrucciones explÃ­citas. Si no hay contexto relevante, el LLM responde: *"No tengo informaciÃ³n sobre ese tema en mis documentos."*

### Fix 3: Umbral de relevancia en retrieval
**Problema:** `rag_system.retrieve()` devolvÃ­a los 3 chunks mÃ¡s cercanos sin importar quÃ© tan irrelevantes fueran.
**SoluciÃ³n:** Filtro de distancia coseno (threshold = 0.6). Chunks con distancia > 0.6 se descartan; si todos son descartados, se devuelve lista vacÃ­a y el LLM rechaza la pregunta.

### Fix 4: Indicadores de carga
**Problema:** Durante los ~2.5s de procesamiento la UI no daba retroalimentaciÃ³n visual.
**SoluciÃ³n:** Dos spinners separados: `ğŸ” Buscando en documentos...` y `âœï¸ Generando respuesta...`

### Fix 5: Instrucciones al usuario actualizadas
**Problema:** El tÃ­tulo decÃ­a "Tutor de lÃ³gica de programaciÃ³n" (dominio del MÃ³dulo 15) y no habÃ­a ejemplos de preguntas vÃ¡lidas.
**SoluciÃ³n:** Nuevo tÃ­tulo "Asistente de GuÃ­as de Estilo de CÃ³digo" + expander con ejemplos de preguntas organizados por dificultad.

---

## ğŸš€ CÃ³mo Usar la AplicaciÃ³n

### Requisitos Previos

```
Python 3.11
Anaconda/Miniconda instalado
Google API Key (Gemini)
```

### InstalaciÃ³n

1. **Descargar el proyecto y entrar a la carpeta:**
   ```bash
   cd proyecto_final
   ```

2. **Activar el entorno conda** (ya configurado):
   ```bash
   conda activate cimaprompter
   ```

3. **Instalar dependencias** (si es la primera vez):
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar API Key:**

   Crear archivo `.env` en la raÃ­z del proyecto:
   ```
   GOOGLE_API_KEY=tu-api-key-aqui
   ```

5. **Construir Knowledge Base** (solo si no existe `chroma_db/`):
   ```bash
   python rag_system.py --rebuild
   ```
   > âš ï¸ Tarda ~10 minutos por el rate limiting de la API gratuita (100 req/min).

### EjecuciÃ³n

```bash
conda activate cimaprompter
cd proyecto_final
streamlit run Cimaprompter_Module17_StreamlitApp.py
```

La app estarÃ¡ disponible en: `http://localhost:8501`

---

## ğŸ’» GuÃ­a de la Interfaz

### 1. PÃ¡gina de Chat ğŸ’¬

**Panel de MÃ©tricas (Superior):**
- â±ï¸ Tiempo promedio de respuesta (meta: < 3s)
- ğŸ’¬ Total de preguntas realizadas
- ğŸ˜Š SatisfacciÃ³n del usuario (% feedback positivo)
- ğŸ“š Tasa de citaciÃ³n (% respuestas con citas reales en texto)

**Columna Izquierda â€” Chat:**
- Escribe tu pregunta en el input
- El sistema busca en los documentos y genera una respuesta con citas

**Columna Derecha â€” Explicabilidad y Fuentes:**
- **ğŸ“š Fuentes Recuperadas:** chunks del knowledge base usados para responder
- **ğŸ“‹ SHAP/LIME:** explicabilidad de la clasificaciÃ³n de tu pregunta
- **ğŸ“Š Feedback:** califica la respuesta con ğŸ‘/ğŸ‘

### 2. PÃ¡gina de Explicabilidad ğŸ”
- AnÃ¡lisis SHAP/LIME de conversaciones anteriores

### 3. PÃ¡gina de RetroalimentaciÃ³n ğŸ“Š
- Dashboard de satisfacciÃ³n acumulada con grÃ¡ficos

### 4. PÃ¡gina de Monitoreo ğŸ“ˆ
- Estado del sistema, cachÃ© y mÃ©tricas tÃ©cnicas

### 5. PÃ¡gina de DocumentaciÃ³n ğŸ“š
- InformaciÃ³n del equipo y stack tecnolÃ³gico

---

## ğŸ§ª Resultados de Tests

> **Fecha de ejecuciÃ³n:** 2026-02-23 | Ambos datasets (M16 y M17) ejecutados y documentados.
> Ver anÃ¡lisis completo en `module17Ans&screenshots/M16_vs_M17_Analysis.md`.

### Pruebas iniciales â€” MÃ³dulo 16 (dataset completo, 10 preguntas)

**Fecha:** 2026-02-23 | **Knowledge Base:** 529 chunks, 3 PDFs

| # | Pregunta (resumida) | Got Answer? | Sources? | Quality | Error | Resp. Time |
|---|---------------------|-------------|----------|---------|-------|------------|
| 1 | IndentaciÃ³n C++ | âœ… Yes | âœ… Yes | Fair | Respuesta parcial | 7.08s |
| 2 | Reglas headings Markdown | âœ… Yes | âœ… Yes | Good | None | 6.72s |
| 3 | Nombres constantes Obj-C | âœ… Yes | âœ… Yes | Good | None | 6.73s |
| 4 | Nombres mÃ©todos C++ vs Obj-C | âœ… Yes | âœ… Yes | Good | None | 7.67s |
| 5 | Comentarios en las 3 guÃ­as | âœ… Yes | âœ… Yes | Good | None | 15.00s |
| 6 | Orden modificadores C++ | âœ… Yes | âœ… Yes | Good | None | 4.50s |
| 7 | Listas anidadas Markdown | âœ… Yes | âœ… Yes | Good | None | 5.79s |
| 8 | Excepciones C++ vs Obj-C | âœ… Yes | âœ… Yes | Fair | Incompleto (C++ faltÃ³) | 10.72s |
| 9 | Bibliotecas ML Python (edge) | âœ… Yes | âœ… Yes | Good | Off-topic con fuentes irrelevantes | 1.84s |
| 10 | ConfiguraciÃ³n IDE (edge) | âœ… Yes | âœ… Yes | Fair | AlucinaciÃ³n parcial | 6.08s |

**Resultado M16:** 7 Good, 3 Fair â†’ **70% pass rate** âœ… | Tiempo promedio: 7.21s

### Resultados completos â€” MÃ³dulo 17 (10 preguntas)

**Fecha:** 2026-02-23 | **Knowledge Base:** 789 chunks, 5 PDFs

| # | Pregunta (resumida) | Got Answer? | Sources? | Quality | Error | Resp. Time |
|---|---------------------|-------------|----------|---------|-------|------------|
| 1 | IndentaciÃ³n C++ | âœ… Yes | âœ… Yes | Fair | Respuesta parcial | ~5.30s |
| 2 | Reglas headings Markdown | âœ… Yes | âœ… Yes | Good | None | 5.32s |
| 3 | Nombres constantes Obj-C | âœ… Yes | âœ… Yes | Good | None | 5.95s |
| 4 | Nombres mÃ©todos C++ vs Obj-C | âœ… Yes | âœ… Yes | Good | None | 5.03s |
| 5 | Comentarios en las 3 guÃ­as | âœ… Yes | âœ… Yes | Fair | False negative (C++ filtrado) | 12.48s |
| 6 | Orden modificadores C++ | âœ… Yes | âœ… Yes | Good | None | 3.51s |
| 7 | Listas anidadas Markdown | âœ… Yes | âœ… Yes | Good | None | â€” |
| 8 | Excepciones C++ vs Obj-C | âœ… Yes | âœ… Yes | Good | None | 9.25s |
| 9 | Bibliotecas ML Python (edge) | âœ… Yes | âŒ No | Good | Off-topic rechazado correctamente | â€” |
| 10 | ConfiguraciÃ³n IDE (edge) | âœ… Yes | âœ… Yes | Fair | ExtrapolaciÃ³n menor | â€” |

**Resultado M17:** 7 Good, 3 Fair â†’ **70% pass rate** âœ… | Tiempo promedio: ~6.26s

### MÃ©tricas finales: M16 vs M17

| MÃ©trica | Target | M16 Resultado | M17 Resultado |
|---------|--------|---------------|---------------|
| Response Time | < 3s* | 7.21s promedio | ~6.26s promedio â¬‡ï¸ |
| Citation Rate | > 80% | 100%â€  | 100% âœ… |
| Success Rate | > 70% | 70% (7/10) âœ… | 70% (7/10) âœ… |
| Off-topic handling | Rechazar | Mostraba fuentes irrelevantes âŒ | Rechaza sin fuentes âœ… |
| Hallucination (edge) | MÃ­nima | Alta (Q10 fabricÃ³ pasos de IDE) | MÃ­nima (solo inferencias razonables) âœ… |
| Multi-doc coverage (Q5) | 3/3 guÃ­as | 3/3 âœ… | 2/3 (false negative C++) âš ï¸ |
| Response length | Conciso | Extenso + preguntas pedagÃ³gicas | Conciso + directo âœ… |

\* Target < 3s aplica a la llamada al LLM (generaciÃ³n). El tiempo total incluye embeddings + retrieval + generaciÃ³n.
â€  La tasa del M16 era correcta por coincidencia â€” el LLM sÃ­ citaba fuentes, pero la mÃ©trica medÃ­a retrieval, no citas reales.

---

## ğŸ“Š AnÃ¡lisis de Resultados

### Lo que funciona bien âœ…

1. **Retrieval efectivo para preguntas dentro del alcance:**
   - ChromaDB recupera chunks relevantes con scores 0.45â€“0.55
   - Top-3 chunks son suficientes para responder correctamente
   - Los tres documentos se recuperan de forma cruzada en preguntas comparativas

2. **IntegraciÃ³n modular sin conflictos:**
   - RAG y SHAP/LIME coexisten sin interferirse
   - MÃ©tricas actualizan en tiempo real sin re-renders problemÃ¡ticos

3. **Performance dentro del target:**
   - Tiempo de respuesta promedio < 3s
   - Caching con `@st.cache_resource` evita recargar modelos

### Lo que mejorÃ³ en MÃ³dulo 17 âœ…

1. **Manejo de off-topic:** Preguntas fuera del alcance ahora reciben rechazo claro
2. **CitaciÃ³n honesta:** La mÃ©trica ahora refleja citas reales, no retrieval
3. **UX:** Spinners de progreso y ejemplos de preguntas visibles desde el inicio

### Ãreas pendientes de mejora ğŸ”®

1. **Ajustar el umbral de relevancia:** El threshold global de 0.6 causÃ³ un false negative en Q5 (chunks de C++ sobre comentarios descartados). Una mejora serÃ­a usar k=5 para queries multi-documento, o un threshold adaptativo.
2. **Streaming:** Respuestas progresivas mejorarÃ­an la UX percibida (`st.write_stream()`).
3. **Tests automatizados:** Actualmente el testing es manual; se podrÃ­a automatizar con pytest.

---

## ğŸ—ï¸ Arquitectura TÃ©cnica

### Stack TecnolÃ³gico

```
Frontend:
  â””â”€ Streamlit 1.54.0 (Multi-page app)

Backend LLM:
  â””â”€ Google Gemini 2.5 Flash
     â””â”€ LangChain 0.3.0 (Orchestration)

RAG System:
  â”œâ”€ Document Loading: PyPDF 6.7.1
  â”œâ”€ Text Splitting: RecursiveCharacterTextSplitter (800 chars, 200 overlap)
  â”œâ”€ Embeddings: Google Gemini Embeddings (gemini-embedding-001)
  â”œâ”€ Vector Store: ChromaDB 0.5.23 (Persistent, cosine distance)
  â”œâ”€ Knowledge Base: 5 PDFs â†’ 789 chunks (C++, Obj-C, Markdown, Python, PEP 8)
  â””â”€ Retrieval: Top-k=3 con filtro de relevancia (threshold=0.6)

Explainability:
  â”œâ”€ LIME 0.2.0.1 (Local interpretability)
  â”œâ”€ SHAP 0.49.1 (Global feature importance)
  â””â”€ Scikit-learn (Local classifier)

Visualization:
  â””â”€ Plotly 6.5.2 (Interactive charts)
```

### Flujo de una consulta

```
Usuario hace pregunta
    â†“
ğŸ” RAG: retrieve(query, k=3, threshold=0.6)
    â†“
Â¿Chunks relevantes encontrados?
    â”œâ”€â”€ NO â†’ LLM responde "No tengo informaciÃ³n sobre ese tema"
    â””â”€â”€ SÃ â†’ Contexto + system prompt estricto â†’ LLM genera respuesta con [Fuente N]
    â†“
Detectar citas en texto â†’ actualizar mÃ©trica de citaciÃ³n
    â†“
SHAP/LIME explica la clasificaciÃ³n de la pregunta
    â†“
Mostrar: Respuesta + Fuentes expandibles + Explicabilidad + MÃ©tricas
```

### Estructura de Archivos

```
proyecto_final/
â”œâ”€â”€ README.md                                    # Este archivo
â”œâ”€â”€ DEPLOYMENT_GUIDE.md                          # GuÃ­a de despliegue
â”œâ”€â”€ QUICKSTART.md                                # Inicio rÃ¡pido
â”œâ”€â”€ requirements.txt                             # Dependencias Python
â”œâ”€â”€ .env                                         # API keys (no en git)
â”‚
â”œâ”€â”€ Cimaprompter_Module17_StreamlitApp.py       # App principal
â”œâ”€â”€ explainability_module.py                     # SHAP/LIME explainer
â”œâ”€â”€ rag_system.py                                # RAG con ChromaDB
â”‚
â”œâ”€â”€ code styles/                                 # Knowledge base (PDFs)
â”‚   â”œâ”€â”€ Google C++ Style Guide.pdf
â”‚   â”œâ”€â”€ Google Objective-C Style Guide _ styleguide.pdf
â”‚   â”œâ”€â”€ Markdown style guide _ styleguide.pdf
â”‚   â”œâ”€â”€ python_style_guide.pdf
â”‚   â””â”€â”€ PEP 8 â€“ Style Guide for Python Code _ peps.python.org.pdf
â”‚
â””â”€â”€ chroma_db/                                   # Vector store (789 chunks, 5 PDFs)
```

---

## ğŸ“ Lo que aprendimos

### MÃ³dulo 16

1. **Rate Limiting de APIs:**
   El tier gratuito de Gemini permite ~100 embeddings/minuto. Para 529 chunks se necesitaron batches con delays de 65s (~10 min total). SoluciÃ³n: procesar en lotes.

2. **Versiones de modelos de embeddings:**
   El modelo `models/embedding-001` ya no estaba disponible. HabÃ­a que usar `models/gemini-embedding-001`. Aprendizaje: siempre verificar con `genai.list_models()`.

3. **Modularidad salva tiempo:**
   Mantener el RAG en un mÃ³dulo separado (`rag_system.py`) permitiÃ³ integrarlo sin tocar el cÃ³digo existente de SHAP/LIME.

### MÃ³dulo 17

4. **Las mÃ©tricas deben medir lo correcto:**
   La tasa de citaciÃ³n marcaba 100% porque medÃ­a retrieval, no citas reales. Una mÃ©trica mal definida da falsa confianza. SoluciÃ³n: buscar patrones de citaciÃ³n en el texto real de la respuesta.

5. **Los LLMs necesitan instrucciones muy explÃ­citas:**
   Decirle al LLM "si no tienes informaciÃ³n, indÃ­calo" no es suficiente â€” lo ignora y usa su conocimiento general. Las instrucciones deben ser directivas, no sugerencias.

6. **El umbral de relevancia es crÃ­tico para RAG:**
   Sin filtro de distancia, el RAG siempre devuelve chunks aunque sean irrelevantes, y el LLM intenta responder con contexto equivocado. El filtro de threshold convierte el RAG en un sistema mÃ¡s honesto.

---

## ğŸ”® Mejoras Futuras

1. **Agregar mÃ¡s guÃ­as de estilo al knowledge base:**
   - âœ… Google Python Style Guide â€” agregado en M17
   - âœ… PEP 8 â€” agregado en M17
   - Google Java Style Guide â€” pendiente (opcional)

2. **Streaming de respuestas:**
   Usar `st.write_stream()` para mostrar la respuesta progresivamente y mejorar la UX percibida.

3. **Soporte multi-idioma:**
   Actualmente el sistema estÃ¡ en espaÃ±ol. PodrÃ­a detectar el idioma de la pregunta y responder en el mismo idioma.

---

## âœ… Checklist MÃ³dulo 17

**Bug fixes & polish:**
- âœ… Al menos 1-2 issues del MÃ³dulo 16 corregidos (se corrigieron 5)
- âœ… La app tiene instrucciones claras para el usuario
- âœ… Error handling previene crashes (try/except en toda la cadena RAGâ†’LLM)
- âœ… Loading indicators muestran progreso durante queries

**Documentation:**
- âœ… README.md con todas las secciones requeridas
- âœ… requirements.txt con todas las dependencias y versiones
- âœ… Setup instructions claras para otro usuario
- âœ… SecciÃ³n "Lo que aprendimos" con reflexiones sobre comportamiento del LLM

**Quality:**
- âœ… App corre sin crashes para queries normales
- âœ… Sources/citations se muestran claramente
- âœ… Al menos 70% de las 10 preguntas funcionan bien (7/10 = 70% â€” ejecutado 2026-02-23)

---

## ğŸ“ Contacto

**Equipo:** Cimaprompter
**InstituciÃ³n:** Universidad AutÃ³noma de Baja California (UABC)

---

## ğŸ“„ Licencia

Este proyecto es parte del curso de IA de la FundaciÃ³n TecnolÃ³gica Iberoamericana.
Desarrollado con fines educativos.

---

*Ãšltima actualizaciÃ³n: 2026-02-23*
*MÃ³dulo: 17 - Complete & Polish*
*Status: âœ… Completo â€” dataset de 10 preguntas ejecutado, 70% pass rate confirmado*

---

**ğŸ¤– Desarrollado con Claude Sonnet 4.5**
